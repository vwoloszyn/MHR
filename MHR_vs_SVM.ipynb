{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "\n",
    "import MHR as mhr\n",
    "\n",
    "def simple_spearman(x,y): return np.abs(spearmanr(x,y)[0])\n",
    "spearmanr_scorer = make_scorer(simple_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reload_package(root_module):\n",
    "    package_name = root_module.__name__\n",
    "\n",
    "    # get a reference to each loaded module\n",
    "    loaded_package_modules = dict([\n",
    "        (key, value) for key, value in sys.modules.items() \n",
    "        if key.startswith(package_name) and isinstance(value, types.ModuleType)])\n",
    "\n",
    "    # delete references to these loaded modules from sys.modules\n",
    "    for key in loaded_package_modules:\n",
    "        del sys.modules[key]\n",
    "\n",
    "    # load each of the modules again; \n",
    "    # make old modules share state with new modules\n",
    "    for key in loaded_package_modules:\n",
    "        print 'loading %s' % key\n",
    "        newmodule = __import__(key)\n",
    "        oldmodule = loaded_package_modules[key]\n",
    "        oldmodule.__dict__.clear()\n",
    "        oldmodule.__dict__.update(newmodule.__dict__)\n",
    "        \n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def calc_ndcg(df, column,k):\n",
    "    min_votes=5\n",
    "    min_comments=30\n",
    "\n",
    "    ndcg_global=[]\n",
    "    grouped=df.groupby('asin')\n",
    "\n",
    "    for name, group in grouped:\n",
    "        dffiltro = (df['asin']==name)\n",
    "        \n",
    "        values_test = df[dffiltro]['helpfulness'].values\n",
    "        scores = df[dffiltro][column].values\n",
    "\n",
    "\n",
    "        ind = (-np.array(scores)).argsort()\n",
    "        a = np.array(values_test)[ind]\t\n",
    "        ndcg = ndcg_at_k(a, k)\n",
    "        ndcg_global.append(ndcg)\n",
    "    return ndcg_global\n",
    "\n",
    "def calc_ndcg_mean(df, column,k):\n",
    "    x = calc_ndcg(df,column,k)\n",
    "    return np.mean(x)\n",
    "\n",
    "def calc_corr(df, column):\n",
    "    correlation=[]\n",
    "    grouped=df.groupby('asin')\n",
    "\n",
    "    for name, group in grouped:\n",
    "        dffiltro = (df['asin']==name)\n",
    "        \n",
    "        helpfulness = df[dffiltro]['helpfulness'].values\n",
    "        scores = df[dffiltro][column].values\n",
    "        correlation.append(np.corrcoef(helpfulness,scores)[0][1])\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (10,11,17,19,20,21,22,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40870, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_features = pd.read_csv('data/eletronic_sample_counts.csv.gz')\n",
    "reviews_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Unnamed: 0.1', u'Unnamed: 0.1.1',\n",
       "       u'Unnamed: 0.1.1', u'MHRs', u'Unnamed: 0.1.1.1', u'Unnamed: 0.1.1.1',\n",
       "       u'Unnamed: 0.1.1.1.1', u'adj', u'asin', u'helpful', u'helpfulness',\n",
       "       u'hits', u'noun', u'overall', u'pageRank', u'pos_tag', u'powerWithStar',\n",
       "       u'reviewText', u'reviewTime', u'reviewerID', u'reviewerName',\n",
       "       u'sentence_count', u'summary', u'tot', u'unigram_count',\n",
       "       u'unixReviewTime', u'word_count', u'pr_hs', u'pr_len', u'hs_len',\n",
       "       u'pr_hs_len', u'revRank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Unnamed: 0.1', u'Unnamed: 0.1.1',\n",
       "       u'Unnamed: 0.1.1', u'MHRs', u'Unnamed: 0.1.1.1', u'Unnamed: 0.1.1.1',\n",
       "       u'Unnamed: 0.1.1.1.1', u'adj', u'helpfulness', u'hits', u'noun',\n",
       "       u'overall', u'pageRank', u'powerWithStar', u'sentence_count', u'tot',\n",
       "       u'unigram_count', u'unixReviewTime', u'word_count', u'pr_hs', u'pr_len',\n",
       "       u'hs_len', u'pr_hs_len', u'revRank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_columns = reviews_features.select_dtypes(include=['float64','int','int64']).columns\n",
    "df_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>unigram_count</th>\n",
       "      <th>adj</th>\n",
       "      <th>noun</th>\n",
       "      <th>helpfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>604</td>\n",
       "      <td>26</td>\n",
       "      <td>292</td>\n",
       "      <td>53</td>\n",
       "      <td>128</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>582</td>\n",
       "      <td>16</td>\n",
       "      <td>273</td>\n",
       "      <td>64</td>\n",
       "      <td>136</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>14</td>\n",
       "      <td>183</td>\n",
       "      <td>22</td>\n",
       "      <td>78</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  sentence_count  unigram_count  adj  noun  helpfulness\n",
       "0          90               2             71    8    20     0.045455\n",
       "1         604              26            292   53   128     0.733333\n",
       "2         582              16            273   64   136     1.000000\n",
       "3         318              14            183   22    78     0.600000\n",
       "4          36               2             32    3     8     0.312500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_features[['word_count','sentence_count','unigram_count','adj','noun','helpfulness']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg at fold 0 = 0.913655280929\n",
      "ndcg at fold 1 = 0.912583503636\n",
      "ndcg at fold 2 = 0.923522562114\n",
      "ndcg at fold 3 = 0.905105137255\n",
      "ndcg at fold 4 = 0.91319186602\n",
      "ndcg at fold 5 = 0.921244709246\n",
      "ndcg at fold 6 = 0.915009124149\n",
      "ndcg at fold 7 = 0.906041944008\n",
      "ndcg at fold 8 = 0.923186490416\n",
      "ndcg at fold 9 = 0.915358528432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#CROSVALIDATION BY COMMENTS\n",
    "clf = svm.SVR()\n",
    "max_ndcg=0\n",
    "bestSVMPrediction=pd.DataFrame()\n",
    "\n",
    "rs = cross_validation.ShuffleSplit(len(reviews_features), n_iter=10, test_size=.1 , random_state=0)\n",
    "ind=0\n",
    "ndcg=[]\n",
    "for train_index, test_index in rs:\n",
    "    #train\n",
    "    features_train = reviews_features.iloc[train_index][list(['word_count','sentence_count','unigram_count','adj','noun'])].values\n",
    "    labels_train = reviews_features.iloc[train_index][\"helpfulness\"].values\n",
    "    clf.fit(features_train, labels_train)\n",
    "    \n",
    "    #test\n",
    "    features_test = reviews_features.iloc[test_index][list(['word_count','sentence_count','unigram_count','adj','noun'])].values\n",
    "    labels_test = reviews_features.iloc[test_index][\"helpfulness\"].values\n",
    "    x=clf.predict(features_test)\n",
    "    \n",
    "    dfTest= reviews_features.iloc[test_index]\n",
    "    dfTest['svm']=x\n",
    "    local_ndcg = calc_ndcg_mean(dfTest,'svm',5)\n",
    "    ndcg.append(local_ndcg)\n",
    "    print \"ndcg at fold \"+str(ind)+\" = \"+ str(local_ndcg)\n",
    "    \n",
    "    ind=ind+1\n",
    "\n",
    "    if (local_ndcg>max_ndcg):\n",
    "        bestSVMPrediction=dfTest\n",
    "    \n",
    "\n",
    "#save bestSVMPrediction on csv\n",
    "bestSVMPrediction.to_csv('data/best_prediction_svm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ndcg@1=0.764161004389\n",
      "SVM ndcg@3=0.873865776305\n",
      "SVM ndcg@5=0.915358528432\n"
     ]
    }
   ],
   "source": [
    "print \"SVM ndcg@1=\"+str(calc_ndcg_mean(bestSVMPrediction,'svm',1))\n",
    "print \"SVM ndcg@3=\"+str(calc_ndcg_mean(bestSVMPrediction,'svm',3))\n",
    "print \"SVM ndcg@5=\"+str(calc_ndcg_mean(bestSVMPrediction,'svm',5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
